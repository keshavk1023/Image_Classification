{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with TensorFlow and Keras\n",
    "This notebook demonstrates an image classification pipeline using TensorFlow and Keras. The process includes data loading, image preprocessing, augmentation, model creation, training, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "import_libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "import os\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "In this section, we load and preprocess the image data. We apply augmentations for better model generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "data_loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images from directory\n",
    "train_data_dir = '/path/to/train_data'\n",
    "test_data_dir = '/path/to/test_data'\n",
    "\n",
    "# Apply ImageDataGenerator for real-time data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load data from directories\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Sample Images\n",
    "Let's visualize a few images from the training dataset to understand the data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "visualize_images",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some images from the training dataset\n",
    "sample_images, sample_labels = next(train_generator)\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "for i in range(5):\n",
    "    axes[i].imshow(sample_images[i])\n",
    "    axes[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "Next, we define the deep learning model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "model_architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer to avoid warnings\n",
    "model.add(Input(shape=(64, 64, 3)))\n",
    "\n",
    "# Add layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))  # Assuming 10 classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "We will now train the model using the training data and validate using the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "train_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=25,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "Let's evaluate the model's performance on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "evaluate_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on validation data\n",
    "test_loss, test_acc = model.evaluate(validation_generator)\n",
    "print(f'Test accuracy: {test_acc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training History\n",
    "We will plot the training and validation accuracy and loss to visualize the model's performance over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "visualize_training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "We will now save the trained model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "save_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('image_classification_model.h5')\n",
    "\n",
    "# Optionally save the model architecture to JSON\n",
    "model_json = model.to_json()\n",
    "with open('image_classification_model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we created and trained an image classification model using TensorFlow and Keras. The model was evaluated and visualized, and we saved the final trained model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
