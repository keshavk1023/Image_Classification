{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Notebook\n",
    "This notebook demonstrates the full pipeline of an image classification task using TensorFlow and Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "import_libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Here we perform the data preprocessing steps like scaling, augmentation, and splitting the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "data_preprocessing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up directories for training and testing\n",
    "train_data_dir = '/path/to/train_data'\n",
    "test_data_dir = '/path/to/test_data'\n",
    "\n",
    "# Data Preprocessing with ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "Next, we build the model architecture. This includes layers like `Dense`, `Flatten`, and `Input`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "model_architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential()\n",
    "\n",
    "# Using Input layer to avoid 'input_shape' warning\n",
    "model.add(Input(shape=(64, 64, 3)))  # Adjust shape based on your dataset\n",
    "\n",
    "# Add layers to the model\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))  # Adjust the number of classes (10 in this case)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "We now train the model using the training and validation data. We will also use early stopping to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "train_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=25,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "We evaluate the trained model on the validation data to check its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "evaluate_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(validation_generator)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Results\n",
    "We plot the accuracy and loss curves to visualize the training and validation performance over the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "visualize_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model\n",
    "Finally, we save the trained model and its architecture to disk for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "save_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('image_classification_model.h5')\n",
    "\n",
    "# Optionally, save the model architecture to JSON\n",
    "model_json = model.to_json()\n",
    "with open('image_classification_model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Prediction\n",
    "Here we show an example of how to load and use the model for making predictions on new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "example_prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using the model for prediction (adjust based on your data)\n",
    "# img_path = '/path/to/single/image.jpg'\n",
    "# img = tf.keras.preprocessing.image.load_img(img_path, target_size=(64, 64))\n",
    "# img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
    "# img_array = np.expand_dims(img_array, axis=0)\n",
    "# prediction = model.predict(img_array)\n",
    "# print(prediction)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
